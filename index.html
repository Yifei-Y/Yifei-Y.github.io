<!DOCTYPE HTML>
<html lang="en">
  
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yifei Yang</title>
  
  <meta name="author" content="Yifei Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/zju_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yifei Yang</name>
              </p>
              <p>I am a fourth-year PhD (Sep. 2022 - ) student in Control Science and Engineering department at <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. I belong to Robotics Lab</a>, advised by Prof. <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a> and <a href="https://ywang-zju.github.io/">Yue Wang</a>.
              </p>
              <p>
                I obtained my B.Eng (Sep. 2018 - Jun. 2022) in Control Science and Engineering from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a> with an honor degree at Chu Kochen Honor College.
              </p>
              <p>
                My current research interests lie in robotic manipulation, robot learning, and embodied AI. My prior work also includes computer vision.
              </p>
              <p style="text-align:center">
                <a href="mailto:yf.yang.research@outlook.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=Pu9UyugAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Yifei-Y">Github</a>
                <!-- <a href="https://twitter.com/KechunXu">Twitter</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/kechun-xu-12aaab1a7/">LinkedIn</a> -->
              </p>
            </td>
            <!-- <td style="padding:2.5%;width:40%;max-width:40%"> 
              <a href="images/Kechun_Xu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kechun_Xu.jpg" class="hoverZoomLink"></a>
              <p style="text-align:center">
                Photo taken by <a href="https://kyleleey.github.io/">Zizhang Li</a>.
              </p>
            </td> --> <!--TODO-->
          </tr>
        </tbody></table>

        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025E2VLA_chen.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2509.14630">
                    <papertitle>Toward Embodiment Equivariant Vision-Language-Action Policy</papertitle>
                </a>
                <br>
                <a>Anzhe Chen</a>,
                <strong>Yifei Yang</strong>,
                <a>Zhenjie Zhu</a>,
                <a href="https://xukechun.github.io">Kechun Xu</a>,
                <a>Zhongxiang Zhou</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>Under Review</em>
                </p>
                <div class="paper" id="Chen2025E2VLA">
                  <a href="https://arxiv.org/abs/2509.14630">arXiv</a> /
                  <a href="https://github.com/hhcaz/e2vla">code</a> 
                </div>
            </td>
        </tr> <!--chen2025e2vla-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025A2C_xu.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2503.09423">
                    <papertitle>Efficient Alignment of Unconditioned Action Prior for Language-conditioned Pick and Place in Clutter</papertitle>
                </a>
                <br>
                <a href="https://xukechun.github.io">Kechun Xu</a>,
                <a>Xunlong Xia</a>,
                <a>Kaixuan Wang</a>,
                <strong>Yifei Yang</strong>,
                <a href="https://yunxuanmao.github.io/">Yunxuan Mao</a>,
                <a>Bing Deng</a>,
                <a href="https://scholar.google.com/citations?user=T9AzhwcAAAAJ&hl=en&oi=ao">Jieping Ye</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE Transactions on Automation Science and Engineering (T-ASE 2025)</em>
                <br>
                <em>Conference on Robot Learning (CoRL 2025), GenPriors Workshop</em>
                </p>
                <div class="paper" id="Xu2025A2">
                  <a href="https://arxiv.org/abs/2503.09423">arXiv</a> /
                  <a href="https://ieeexplore.ieee.org/document/11152358">ieee</a> /
                  <a href="https://xukechun.github.io/papers/A2">project</a> /
                  <a href="https://github.com/xukechun/Action-Prior-Alignment">code</a> /
                  <a href="https://www.bilibili.com/video/BV1dPX4YzEzk/?spm_id_from=333.1391.0.0">video</a>
                </div>
            </td>
        </tr> <!--xu2025a2-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025PseudoTactile_yang.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2503.23835">
                    <papertitle>Disambiguate Gripper State in Grasp-Based Tasks: Pseudo-Tactile as Feedback Enables Pure Simulation Learning</papertitle>
                </a>
                <br>
                <strong>Yifei Yang</strong>,
                <a>Lu Chen</a>,
                <a>Zherui Song</a>,
                <a>Yenan Chen</a>,
                <a>Wentao Sun</a>,
                <a>Zhongxiang Zhou</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</em>
                </p>
                <div class="paper" id="Yang2025PseudoTactile">
                  <a href="https://arxiv.org/abs/2503.23835">arXiv</a> /
                  <a href="https://yifei-y.github.io/project-pages/Pseudo-Tactile-Feedback/">project</a> /
                  <a href="https://www.bilibili.com/video/BV1DZZxYGEk6/?vd_source=16bffa885f8d40c0678b340384dd56db">video</a>
                </div>
            </td>
        </tr> <!--yang2025pseudotactile-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2024DORec_wu.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2310.11092">
                    <papertitle>DORec: Decomposed Object Reconstruction and Segmentation Utilizing 2D Self-Supervised Features</papertitle>
                </a>
                <br>
                <a>Jun Wu</a>,
                <a>Sicheng Li</a>,
                <a>Sihui Ji</a>,
                <strong>Yifei Yang</strong>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://yiyiliao.github.io">Yiyi Liao</a>
                <br>
                <em>IEEE Robotics and Automation Letters (RA-L 2024)</em>
                </p>
                <div class="paper" id="Wu2024DORec">
                  <a href="https://arxiv.org/abs/2310.11092">arXiv</a> /
                  <a href="https://ieeexplore.ieee.org/document/10777610">ieee</a>
                </div>
            </td>
        </tr> <!--Wu2024DORec-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2024nuDBA_mao.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2404.18439">
                    <papertitle>&nu;-DBA: Neural Implicit Dense Bundle Adjustment Enables Image-Only Driving Scene Reconstruction</papertitle>
                </a>
                <br>
                <a href="https://yunxuanmao.github.io/">Yunxuan Mao</a>,
                <a>Bingqi Shen</a>,
                <strong>Yifei Yang</strong>,
                <a>Kai Wang</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://yiyiliao.github.io">Yiyi Liao</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)</em>
                </p>
                <div class="paper" id="Mao2024nuDBA">
                  <a href="https://arxiv.org/abs/2404.18439">arXiv</a> /
                  <a href="https://ieeexplore.ieee.org/document/10801847">ieee</a>
                </div>
            </td>
        </tr> <!--mao2024nudba-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2024SemSeg_yang.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/document/10388394">
                    <papertitle>Class Semantics Modulation for Open-Set Instance Segmentation</papertitle>
                </a>
                <br>
                <strong>Yifei Yang</strong>,
                <a>Zhongxiang Zhou</a>,
                <a>Jun Wu</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                <br>
                <em>IEEE Robotics and Automation Letters (RA-L 2024)</em>
                </p>
                <div class="paper" id="Yang2024SemSeg">
                  <a href="https://ieeexplore.ieee.org/document/10388394">ieee</a> /
                  <a href="https://yifei-y.github.io/project-pages/SemSeg/">project</a> /
                  <a href="https://github.com/Yifei-Y/SemSeg">code</a>
                </div>
            </td>
        </tr> <!--yang2024semseg-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2023OpensetRCNN_zhou.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2211.11530">
                    <papertitle>Open-Set Object Detection Using Classification-free Object Proposal and Instance-level Contrastive Learning</papertitle>
                </a>
                <br>
                <a>Zhongxiang Zhou</a>
                <strong>Yifei Yang</strong>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                <br>
                <em>IEEE Robotics and Automation Letters (RA-L 2023)</em>
                </p>
                <div class="paper" id="Zhou2023OSRCNN">
                  <a href="https://arxiv.org/abs/2211.11530">arXiv</a> /
                  <a href="https://ieeexplore.ieee.org/document/10035923">ieee</a> /
                  <a href="https://sites.google.com/view/openset-rcnn/">project</a> /
                  <a href="https://github.com/Yifei-Y/Openset-RCNN">code</a>
                </div>
            </td>
        </tr> <!--zhou2023osrcnn-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2023UrbanGIRAFFE_yang.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2303.14167">
                    <papertitle>UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields</papertitle>
                </a>
                <br>
                <a>Yuanbo Yang</a>,
                <strong>Yifei Yang</strong>,
                <a>Hanlei Guo</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                <a href="https://yiyiliao.github.io">Yiyi Liao</a>
                <br>
                <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV 2023)</em>
                </p>
                <div class="paper" id="Yang2023UrbanGIRAFFE">
                  <a href="https://arxiv.org/abs/2303.14167">arXiv</a> /
                  <a href="https://lv3d.github.io/urbanGIRAFFE/">project</a> /
                  <a href="https://github.com/freemty/urbanGIRAFFE">code</a>
                </div>
            </td>
        </tr> <!--yang2023urbangiraffe-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2022ControlVAE_shao.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shao_Rethinking_Controllable_Variational_Autoencoders_CVPR_2022_paper.pdf">
                    <papertitle>Rethinking Controllable Variational Autoencoders</papertitle>
                </a>
                Huajie Shao1*† , Yifei Yang2† , Haohong Lin3† , Longzhong Lin2, Yizhuo Chen2, Qinmin Yang2, Han Zhao
                <br>
                <a href="https://huajieshao.github.io">Huajie Shao*</a>,
                <strong>Yifei Yang*</strong>,
                <a href="https://hhlin.info">Haohong Lin*</a>,
                <a>Longzhong Lin</a>,
                <a>Yizhuo Chen</a>,
                <a>Qinmin Yang</a>,
                <a>Han Zhao</a>
                <br>
                <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)</em>
                </p>
                <div class="paper" id="Shao2022ControlVAE"></div>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shao_Rethinking_Controllable_Variational_Autoencoders_CVPR_2022_paper.pdf">pdf</a>
                </div>
            </td>
        </tr> <!--shao2022controlvae-->

          </tbody></table>

          <hr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Services</heading>
                </td>
            </tr>
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <ul>
                <li>Reviewer of <b>RA-L</b>, <b>ICRA</b>, <b>IROS</b>, <b>CVPR</b>.</li>
              </ul>
            </td>
          </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">Template from <a href="https://jonbarron.info/">Jon Barron</a></p>
              </td>
            </tr>
          </tbody></table>


      </td>
    </tr>
    <!-- <tr style="padding:200px">
      <script type='text/javascript' id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=4z8V1gNS4ZaZr3c7zjIyBg_Zqm3bdY0PoDD9m7VD8R0'></script>
    </tr> -->

    

  </table>
</body>

</html>